{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import binom, binom_test\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod import families\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_street = pd.read_pickle(r'file1')\n",
    "predicted_pension = pd.read_pickle(r'file2')\n",
    "predicted_nonpension = pd.read_pickle(r'file3')\n",
    "\n",
    "actual_street =  pd.read_pickle(r\"file4\")\n",
    "actual_pension =  pd.read_pickle(r\"file5\")\n",
    "actual_nonpension =  pd.read_pickle(r\"file6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prep(predicted, actual):\n",
    "    predicted = predicted.iloc[:,1:]\n",
    "    predicted['CONTRACT_REF_NO'] = predicted['CONTRACT_REF_NO'].str.split('_', 1).str[0]\n",
    "    predicted = predicted.rename(columns = {'CONTRACT_REF_NO':'CONTRACT_REF_NO_raw'})\n",
    "    df_merged = predicted.merge(actual, on=['CONTRACT_REF_NO_raw'])\n",
    "    df_merged = df_merged.rename(columns = {'CONTRACT_REF_NO_raw':'obs','final_pd':'pd','bad_flag_y':'bad_flag'})\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_street = df_prep(predicted_street, actual_street)\n",
    "df_merged_pension = df_prep(predicted_pension, actual_pension)\n",
    "df_merged_nonpension = df_prep(predicted_nonpension, actual_nonpension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 2 — Linearity of independent variables and log-odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the critical assumptions of logistic regression is that the relationship between the logit (aka log-odds) of the outcome and each continuous independent variable is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_var = ['pd']\n",
    "\n",
    "def assump2(df_merged):\n",
    "    # Add logit transform interaction terms (natural log) for continuous variables e.g.. Age * Log(Age)\n",
    "    for var in continuous_var:\n",
    "        df_merged[f'{var}:Log_{var}'] = df_merged[var].apply(lambda x: x * np.log(x))\n",
    "\n",
    "    # Keep columns related to continuous variables\n",
    "    cols_to_keep = continuous_var + df_merged.columns.tolist()[-len(continuous_var):]\n",
    "\n",
    "    # Redefining variables to include interaction terms\n",
    "    X_lt = df_merged[cols_to_keep]\n",
    "    X_lt_constant = sm.add_constant(X_lt, prepend=False)\n",
    "    y = df_merged['bad_flag']\n",
    "    return X_lt_constant, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lt_constant_street, y_street = assump2(df_merged_street)\n",
    "X_lt_constant_pension, y_pension = assump2(df_merged_pension)\n",
    "X_lt_constant_nonpension, y_nonpension = assump2(df_merged_nonpension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREET\n",
    "# Building model and fit the data (using statsmodel's Logit)\n",
    "logit_results_street = GLM(y_street, X_lt_constant_street, family=families.Binomial()).fit()\n",
    "# Display summary results\n",
    "print(logit_results_street.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PENSION\n",
    "# Building model and fit the data (using statsmodel's Logit)\n",
    "logit_results_pension = GLM(y_pension, X_lt_constant_pension, family=families.Binomial()).fit()\n",
    "# Display summary results\n",
    "print(logit_results_pension.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NON-PENSION\n",
    "# Building model and fit the data (using statsmodel's Logit)\n",
    "logit_results_nonpension = GLM(y_nonpension, X_lt_constant_nonpension, family=families.Binomial()).fit()\n",
    "# Display summary results\n",
    "print(logit_results_nonpension.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pd:Log_pd**  is statistically significant (i.e., p≤0.05), indicating the presence of non-linearity between Lod_pd and the logit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to perform transformations by incorporating higher-order polynomial terms to capture the non-linearity **(e.g., Log_pd²)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_nonpension.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_street = df_merged_street[['rm_3_sum_Total_12', 'mS_ever_01d_all_all_09',\n",
    "       'vm_010_o_prin_01d_all_all_12', 'Age_gender', 'Sektor', 'pd']]\n",
    "df_merged_pension = df_merged_pension[['vm_100_o_sal_sal_09', 'mS_ever_15d_all_all_06', 'Age_gender', 'Region',\n",
    "       'pd']]\n",
    "df_merged_nonpension = df_merged_nonpension[['rm_3_sum_Total_12', 'mS_ever_15d_all_all_12',\n",
    "       'vm_010_o_tota_01d_all_all_12', 'Age_gender', 'Ministry', 'pd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dependent and independent variables\n",
    "# X_street = df_merged_street[['pd','Age_gender','Sektor','rm_3_sum_Total_12','vm_010_o_prin_01d_all_all_12','mS_ever_01d_all_all_09']]\n",
    "# X_pension = df_merged_pension[['pd','Age_gender_pred_prob','Region_pred_prob','mS_ever_15d_all_all_06_pd_pred_prob','vm_100_o_sal_sal_09_pred_prob']]\n",
    "# X_nonpension = df_merged_nonpension[['pd','Age_gender_pred_prob','Ministry_pd_pred_prob','rm_3_sum_Total_12_pred_prob','mS_ever_15d_all_all_12_pd_pred_prob','vm_010_o_tota_01d_all_all_12_pd']]\n",
    "X_street = df_merged_street.copy()\n",
    "X_pension = df_merged_pension.rename(columns = {'Age_gender':'Age_gender_pred_prob','mS_ever_15d_all_all_06':'mS_ever_15d_all_all_06_pd_pred_prob','vm_100_o_sal_sal_09':'vm_100_o_sal_sal_09_pred_prob'})\n",
    "X_nonpension = df_merged_nonpension.rename(columns = {'Age_gender':'Age_gender_pred_prob','Ministry':'Ministry_pd_pred_prob','rm_3_sum_Total_12':'rm_3_sum_Total_12_pred_prob','mS_ever_15d_all_all_12':'mS_ever_15d_all_all_12_pd_pred_prob','vm_010_o_tota_01d_all_all_12':'vm_010_o_tota_01d_all_all_12_pd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_constant_street = sm.add_constant(X_street, prepend=False)\n",
    "X_constant_pension = sm.add_constant(X_pension, prepend=False)\n",
    "X_constant_nonpension = sm.add_constant(X_nonpension, prepend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_results(X_constant, y):\n",
    "    # Re-running logistic regression on the original set of X and y variables\n",
    "    logit_results = GLM(y, X_constant, family=families.Binomial()).fit()\n",
    "    predicted = logit_results.predict(X_constant)\n",
    "\n",
    "    # Getting log odds values\n",
    "    log_odds = np.log(predicted / (1 - predicted))\n",
    "\n",
    "    # Visualize predictor variable vs logit values for Age\n",
    "    plt.scatter(x=X_constant['pd'].values, y=log_odds)\n",
    "    plt.xlabel(\"pd\")\n",
    "    plt.ylabel(\"Log-odds\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results(X_constant_street, y_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results(X_constant_pension, y_pension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results(X_constant_nonpension, y_nonpension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above scatter plot shows a clear non-linear pattern of **pd** vs. the log-odds, thereby implying that the assumption of logit linearity is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try only with pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pd_street = df_merged_street['pd']\n",
    "X_pd_pension = df_merged_pension['pd']\n",
    "X_pd_nonpension = df_merged_nonpension['pd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pd_constant_street = sm.add_constant(X_pd_street, prepend=False)\n",
    "X_pd_constant_pension = sm.add_constant(X_pd_pension, prepend=False)\n",
    "X_pd_constant_nonpension = sm.add_constant(X_pd_nonpension, prepend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results(X_pd_constant_street, y_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results(X_pd_constant_pension, y_pension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results(X_pd_constant_nonpension, y_nonpension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 3— No strongly influential outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression assumes that there are no highly influential outlier data points, as they distort the outcome and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not all outliers are influential observations. Rather, outliers have the potential to be influential. To assess this assumption, we need to check whether both criteria are satisfied, i.e., influential and outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Cook’s Distance to determine the influence of a data point, and it is calculated based on its residual and leverage. It summarizes the changes in the regression model when that particular (ith) observation is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One standard threshold is 4/N (where N = number of observations), meaning that observations with Cook’s Distance > 4/N are deemed as influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM method for logreg here so that we can retrieve the influence measures\n",
    "logit_model_street = GLM(y_street, X_constant_street, family=families.Binomial())\n",
    "logit_results_street= logit_model_street.fit()\n",
    "print(\"---------------------------------STREET------------------------------------\")\n",
    "print(logit_results_street.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM method for logreg here so that we can retrieve the influence measures\n",
    "logit_model_pension = GLM(y_pension, X_constant_pension, family=families.Binomial())\n",
    "logit_results_pension= logit_model_pension.fit()\n",
    "print(\"---------------------------------PENSION---------------------------------\")\n",
    "print(logit_results_pension.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM method for logreg here so that we can retrieve the influence measures\n",
    "logit_model_nonpension = GLM(y_nonpension, X_constant_nonpension, family=families.Binomial())\n",
    "logit_results_nonpension= logit_model_nonpension.fit()\n",
    "print(\"-------------------------------NON-PENSION-------------------------------\")\n",
    "print(logit_results_nonpension.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assump3(logit_results, X):\n",
    "    # Get influence measures\n",
    "    influence = logit_results.get_influence()\n",
    "\n",
    "    # Obtain summary df of influence measures\n",
    "    summ_df = influence.summary_frame()\n",
    "    diagnosis_df = summ_df.loc[:,['cooks_d']]\n",
    "\n",
    "    # Append absolute standardized residual values\n",
    "    diagnosis_df['std_resid'] = stats.zscore(logit_results.resid_pearson)\n",
    "    diagnosis_df['std_resid'] = diagnosis_df.loc[:,'std_resid'].apply(lambda x: np.abs(x))\n",
    "\n",
    "    # Sort by Cook's Distance\n",
    "    diagnosis_df.sort_values(\"cooks_d\", ascending=False)\n",
    "    display(diagnosis_df)\n",
    "\n",
    "    # Set Cook's distance threshold\n",
    "    cook_threshold = 4 / len(X)\n",
    "    print(f\"Threshold for Cook Distance = {cook_threshold}\")\n",
    "\n",
    "    # Plot influence measures (Cook's distance)\n",
    "    fig = influence.plot_index(y_var=\"cooks\", threshold=cook_threshold)\n",
    "    plt.axhline(y=cook_threshold, ls=\"--\", color='red')\n",
    "    fig.tight_layout(pad=2)\n",
    "\n",
    "    # Find number of observations that exceed Cook's distance threshold\n",
    "    outliers = diagnosis_df[diagnosis_df['cooks_d'] > cook_threshold]\n",
    "    prop_outliers = round(100*(len(outliers) / len(X)),1)\n",
    "    print(f'Proportion of data points that are highly influential = {prop_outliers}%')\n",
    "\n",
    "    # Find number of observations which are BOTH outlier (std dev > 3) and highly influential\n",
    "    extreme = diagnosis_df[(diagnosis_df['cooks_d'] > cook_threshold) & \n",
    "                        (diagnosis_df['std_resid'] > 3)]\n",
    "    prop_extreme = round(100*(len(extreme) / len(X)),1)\n",
    "    print(f'Proportion of highly influential outliers = {prop_extreme}%')\n",
    "\n",
    "    # Display top 5 most influential outliers\n",
    "    display(extreme.sort_values(\"cooks_d\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assump3(logit_results_street, X_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assump3(logit_results_pension, X_pension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assump3(logit_results_nonpension, X_nonpension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try only with pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM method for logreg here so that we can retrieve the influence measures\n",
    "logit_model_pd_street = GLM(y_street, X_pd_constant_street, family=families.Binomial())\n",
    "logit_results_pd_street = logit_model_pd_street.fit()\n",
    "print(logit_results_pd_street.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM method for logreg here so that we can retrieve the influence measures\n",
    "logit_model_pd_pension = GLM(y_pension, X_pd_constant_pension, family=families.Binomial())\n",
    "logit_results_pd_pension = logit_model_pd_pension.fit()\n",
    "print(logit_results_pd_pension.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM method for logreg here so that we can retrieve the influence measures\n",
    "logit_model_pd_nonpension = GLM(y_nonpension, X_pd_constant_nonpension, family=families.Binomial())\n",
    "logit_results_pd_nonpension = logit_model_pd_nonpension.fit()\n",
    "print(logit_results_pd_nonpension.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assump3(logit_results_pd_street, X_pd_constant_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 3508 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_street.pd.iloc[89296]))\n",
    "print(\"Bad flag: \"+ str(y_street.iloc[89296]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 3405 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_street.pd.iloc[12586]))\n",
    "print(\"Bad flag: \"+ str(y_street.iloc[12586]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 652 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_street.pd.iloc[74892]))\n",
    "print(\"Bad flag: \"+ str(y_street.iloc[74892]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assump3(logit_results_pd_pension, X_pd_constant_pension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 46000 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_pension.pd.iloc[115418]))\n",
    "print(\"Bad flag: \"+ str(y_pension.iloc[115418]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 51774 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_pension.pd.iloc[68296]))\n",
    "print(\"Bad flag: \"+ str(y_pension.iloc[68296]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 58310 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_pension.pd.iloc[69903]))\n",
    "print(\"Bad flag: \"+ str(y_pension.iloc[69903]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assump3(logit_results_pd_nonpension, X_pd_constant_nonpension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 58672 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_nonpension.pd.iloc[46932]))\n",
    "print(\"Bad flag: \"+ str(y_nonpension.iloc[46932]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 23751 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_nonpension.pd.iloc[34336]))\n",
    "print(\"Bad flag: \"+ str(y_nonpension.iloc[34336]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into index 36100 (extreme outlier)\n",
    "print(\"Prob: \"+ str(X_pd_constant_nonpension.pd.iloc[201590]))\n",
    "print(\"Bad flag: \"+ str(y_nonpension.iloc[201590]))  # 1 = Default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is important to note that for data points with relative high Cook's distances, it does not automatically mean that it should be immediately removed from the dataset. It is essentially an indicator to highlight which data points are worth looking deeper into, to understand whether they are true anomalies or not\n",
    "In practice, an assessment of “large” values is a judgement call based on experience and the particular set of data being analyzed.\n",
    "In addition, based on our pre-defined threshold (4/N), only 5% of the points are in the outlier zone, which is small as well. The issue comes when there is a significant number of data points classified as outliers.\n",
    "The management of outliers is outside the scope of this demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 4 — Absence of Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it  is okay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 5— Independence of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations must be independent of each other, i.e., they should not come from repeated or paired data. This means that each observation is not influenced by or related to the rest of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check residuals series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_series(logit_results, X):\n",
    "    # Generate residual series plot\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    ax = fig.add_subplot(111, title=\"Residual Series Plot\",\n",
    "                        xlabel=\"Index Number\", ylabel=\"Deviance Residuals\")\n",
    "\n",
    "    # ax.plot(X_pd.index.tolist(), stats.zscore(logit_results.resid_pearson))\n",
    "    ax.plot(X.index.tolist(), stats.zscore(logit_results.resid_deviance))\n",
    "    plt.axhline(y=0, ls=\"--\", color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_series(logit_results_pd_street, X_pd_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_series(logit_results_pd_pension, X_pd_pension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_series(logit_results_pd_nonpension, X_pd_nonpension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the residuals in the plot above appear to be randomly scattered around the centerline of zero, we can infer (visually) that the assumption is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All assumptions are satisfied"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
